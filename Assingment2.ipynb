{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xC5wvidAYpzc",
        "outputId": "12a38cb8-6e05-402e-f35e-59cf9df1a54f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 812
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting talos\n",
            "  Downloading talos-1.3-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.4/56.4 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting astetik\n",
            "  Downloading astetik-1.13-py3-none-any.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sklearn\n",
            "  Downloading sklearn-0.0.post4.tar.gz (3.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1f623fcee1a3>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install talos'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtalos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'talos'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*- In TensorFlow\n",
        "\"\"\"Assignment2_tensor.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1UjqhL-oDzOuvfZ7wKzKS7Qkx9WFRSWWm\n",
        "\"\"\"\n",
        "\n",
        "!pip install talos\n",
        "import talos\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras import backend\n",
        "from tensorflow.keras.activations import softmax, sigmoid, relu\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import pickle\n",
        "from talos import Scan, Evaluate, Deploy\n",
        "\n",
        "with open('train', 'rb') as file:\n",
        "    train_dict = pickle.load(file, encoding='bytes')\n",
        "\n",
        "with open('test', 'rb') as file:\n",
        "    test_dict = pickle.load(file, encoding='bytes')\n",
        "\n",
        "X_train = train_dict[b'data']\n",
        "y_train = train_dict[b'coarse_labels']\n",
        "\n",
        "X_test = test_dict[b'data']\n",
        "y_test = test_dict[b'coarse_labels']\n",
        "\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 100)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 100)\n",
        "\n",
        "p = {\n",
        "    'units': [120, 240],\n",
        "    'hidden_activations': ['relu', 'sigmoid'],\n",
        "    'activation': ['softmax', 'sigmoid'],\n",
        "    'loss': ['mse', 'categorical_crossentropy'],\n",
        "    'optimizer': ['adam', 'adagrad'],\n",
        "    'batch_size': [1000, 2000]\n",
        "}\n",
        "\n",
        "def create_model(X_train,y_train,X_test,y_test,params):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(units=params['units'], input_shape = X_train.shape[1:]))\n",
        "  model.add(Dense(units=params['units'], activation=params['hidden_activations']))\n",
        "  model.add(Dense(units=params['units'], activation=params['hidden_activations']))\n",
        "  model.add(Dense(units=params['units'], activation=params['hidden_activations']))\n",
        "  model.add(Dense(units=params['units'], activation=params['hidden_activations']))\n",
        "  model.add(Dense(units=params['units'], activation=params['hidden_activations']))\n",
        "  model.add(Dense(units=100, activation=params['activation']))\n",
        "\n",
        "  model.compile(loss=params['loss'], optimizer=params['optimizer'], metrics=['accuracy'])\n",
        "\n",
        "  out = model.fit(x=X_train, y=y_train, batch_size=params['batch_size'], epochs=200)\n",
        "\n",
        "  return out,model\n",
        "\n",
        "t = talos.Scan(x=X_train, y=y_train, params=p, model=create_model, x_val=X_test, y_val=y_test, experiment_name=\"talos_output\")\n",
        "#e = talos.Evaluate(t)\n",
        "#evaluation = e.evaluate(X_test, y_test, metric = 'accuracy')\n",
        "#model = Deploy(t, model_name = 'CIFAR100', metric = 'accuracy')\n",
        "\n",
        "#t = talos.Scan(x=X_train, y=y_train, params=p, model=create_model, x_val=X_test, y_val=y_test, experiment_name=\"talos_output\")\n",
        "e = talos.Evaluate(t)\n",
        "print(\"Evaluation\")\n",
        "#result = model.evaluate(X_test, y_test)\n",
        "evaluation = e.evaluate(X_test, y_test, metric = 'accuracy', task = 'multi_label')\n",
        "print(evaluation[0])\n",
        "print(evaluation[1])\n",
        "print(evaluation[2])\n",
        "print(evaluation[3])\n",
        "print(evaluation[4])\n",
        "model = Deploy(t, model_name = 'CIFAR100', metric = 'accuracy')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-   In PyTORCH\n",
        "\"\"\"Assingment2_Pytorch.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1uZ0K0E5PVM-RQ_od99KGpBcmpJlw0RXv\n",
        "\"\"\"\n",
        "\n",
        "!pip install pytorch_lightning\n",
        "!pip install talos\n",
        "import talos\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import TensorDataset\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "import pickle\n",
        "from pytorch_lightning import Trainer, seed_everything, LightningModule\n",
        "from pytorch_lightning.callbacks import EarlyStopping\n",
        "from talos import Scan, Evaluate, Deploy\n",
        "\n",
        "with open('train', 'rb') as file:\n",
        "    train_dict = pickle.load(file, encoding='bytes')\n",
        "\n",
        "with open('test', 'rb') as file:\n",
        "    test_dict = pickle.load(file, encoding='bytes')\n",
        "\n",
        "X_train = train_dict[b'data']\n",
        "y_train = train_dict[b'coarse_labels']\n",
        "\n",
        "X_test = test_dict[b'data']\n",
        "y_test = test_dict[b'coarse_labels']\n",
        "\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "y_train = y_train.view(-1, 1) # reshape to column tensor\n",
        "y_train = y_train.expand(-1, 100) # replicate the column tensor to a matrix of size [batch_size, 100]\n",
        "\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
        "y_test = y_test.view(-1, 1)\n",
        "y_test = y_test.expand(-1, 100)\n",
        "\n",
        "#y_train = torch.tensor(y_train, dtype=torch.int64)\n",
        "#y_test = torch.tensor(y_test, dtype=torch.int64)\n",
        "\n",
        "train_dataset = TensorDataset(torch.tensor(X_train), y_train)\n",
        "test_dataset = TensorDataset(torch.tensor(X_test), y_test)\n",
        "\n",
        "p = {\n",
        "    'hidden_dim': [120, 240],\n",
        "    'hidden_activations': [F.relu, F.sigmoid],\n",
        "    'output_activation': [F.softmax, F.sigmoid],\n",
        "    'loss': [F.mse_loss, torch.nn.CrossEntropyLoss()],\n",
        "    'optimizer': [torch.optim.Adam, torch.optim.Adagrad],\n",
        "    'batch_size': [1000, 2000]\n",
        "}\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, hidden_dim, hidden_activation, output_activation):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(3072, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc5 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc6 = nn.Linear(hidden_dim, 100)\n",
        "        self.hidden_activation = hidden_activation\n",
        "        self.output_activation = output_activation\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 3072)  # flatten the input tensor\n",
        "        x = self.hidden_activation(self.fc1(x))\n",
        "        x = self.hidden_activation(self.fc2(x))\n",
        "        x = self.hidden_activation(self.fc3(x))\n",
        "        x = self.hidden_activation(self.fc4(x))\n",
        "        x = self.hidden_activation(self.fc5(x))\n",
        "        x = self.output_activation(self.fc6(x))\n",
        "        return x\n",
        "\n",
        "#def create_model(X_train, y_train, X_test, y_test, params):\n",
        " #   net = Net(params['hidden_dim'], params['hidden_activations'], params['output_activation'])\n",
        "  #  criterion = params['loss']\n",
        "   # optimizer = params['optimizer'](net.parameters())\n",
        "  #  batch_size = params['batch_size']\n",
        "  #  train_dataset = TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\n",
        " #   train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "#    for epoch in range(200):\n",
        "#        running_loss = 0.0\n",
        "  #      for i, data in enumerate(train_loader, 0):\n",
        " #           inputs, labels = data\n",
        "  #          optimizer.zero_grad()\n",
        "  #          outputs = net(inputs)\n",
        "  #          loss = criterion(outputs, labels)\n",
        " #           loss.backward()\n",
        " #           optimizer.step()\n",
        " #           running_loss += loss.item()\n",
        "#            print('[Epoch %d] loss: %.3f' % (epoch + 1, running_loss / len(train_loader)))\n",
        "#    return net\n",
        "\n",
        "    # Use GPU if available\n",
        "#    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "#    net.to(device)\n",
        "def create_model(X_train, y_train, X_test, y_test, params):\n",
        "    net = Net(params['hidden_dim'], params['hidden_activations'], params['output_activation'])\n",
        "    criterion = params['loss']\n",
        "    optimizer = params['optimizer'](net.parameters())\n",
        "    batch_size = params['batch_size']\n",
        "    train_dataset = TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "    for epoch in range(200):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            #print('[Epoch %d] loss: %.3f' % (epoch + 1, running_loss / len(train_loader)))\n",
        "\n",
        "    return net\n",
        "\n",
        "t = talos.Scan(x=X_train, y=y_train, params=p, model=create_model, x_val=X_test, y_val=y_test, experiment_name=\"talos_output\")\n",
        "e = talos.Evaluate(t)\n",
        "evaluation = e.evaluate(X_test, y_test, metric = 'accuracy')\n",
        "model = Deploy(t, model_name = 'CIFAR100', metric = 'accuracy')"
      ],
      "metadata": {
        "id": "ishzFINtZH7M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}